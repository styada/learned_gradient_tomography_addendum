{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially learned gradient descent without regularizer as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code implements a partially learned gradient descent algorithm without regularizer for solving\n",
    "an inverse problem using TensorFlow and ODL.\n",
    "\n",
    ":param validation: The `validation` parameter in the code is used to determine whether to generate a\n",
    "set of random data for validation purposes. When `validation` is set to `True`, the code generates a\n",
    "single set of data for validation. Otherwise, it generates multiple sets of random data for\n",
    "training, defaults to False (optional)\n",
    ":return: The code is a TensorFlow implementation of a partially learned gradient descent\n",
    "algorithm without a regularizer as input. It involves creating ODL (Operator Discretization Library)\n",
    "data structures, generating random data, defining placeholders and variables, implementing an\n",
    "iterative scheme, calculating loss, defining an optimizer, and training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import odl\n",
    "\n",
    "# odl.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import odl\n",
    "import odl.contrib.tensorflow\n",
    "from util import random_phantom, conv2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet is creating ODL (Operator Discretization Library) data structures for tomographic imaging. \n",
    "# Here's a breakdown of what each part of the code is doing:\n",
    "# Create ODL data structures\n",
    "size = 128\n",
    "space = odl.uniform_discr([-64, -64], [64, 64], [size, size], dtype='float32')\n",
    "\n",
    "# The code snippet is setting up the geometry, operator, and pseudoinverse operator for tomographic imaging using the ODL library.\n",
    "geometry = odl.tomo.parallel_beam_geometry(space, num_angles=30)\n",
    "operator = odl.tomo.RayTransform(space, geometry)\n",
    "pseudoinverse = odl.tomo.fbp_op(operator)\n",
    "\n",
    "# Ensure operator has fixed operator norm for scale invariance\n",
    "opnorm = odl.power_method_opnorm(operator)\n",
    "operator = (1 / opnorm) * operator\n",
    "pseudoinverse = pseudoinverse * opnorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ODLTransformLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, odl_operator, name='ODLTransform', **kwargs):\n",
    "#         super().__init__(name=name, **kwargs)   \n",
    "#         self.odl_operator = odl_operator\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # Use tf.py_function to apply the ODL operator within TensorFlow\n",
    "#         def numpy_func(x):\n",
    "#             # Convert Tensor to numpy and apply the ODL operator element-wise\n",
    "#             x_np = x.numpy()  # Ensure conversion from TensorFlow tensor to numpy\n",
    "#             result = np.array([self.odl_operator(xi) for xi in x_np])\n",
    "#             return result\n",
    "\n",
    "#         # Wrap the numpy_func using tf.py_function\n",
    "#         output = tf.py_function(func=numpy_func, inp=[inputs], Tout=inputs.dtype)\n",
    "\n",
    "#         # Set the shape explicitly to maintain TensorFlow's shape inference\n",
    "#         output.set_shape(inputs.shape)\n",
    "#         return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'tensorflow_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m odl_op_layer \u001b[38;5;241m=\u001b[39m odl\u001b[38;5;241m.\u001b[39mcontrib\u001b[38;5;241m.\u001b[39mtensorflow\u001b[38;5;241m.\u001b[39mas_tensorflow_layer(operator,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRayTransform\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m odl_op_layer_adjoint \u001b[38;5;241m=\u001b[39m odl\u001b[38;5;241m.\u001b[39mcontrib\u001b[38;5;241m.\u001b[39mtensorflow\u001b[38;5;241m.\u001b[39mas_tensorflow_layer(operator\u001b[38;5;241m.\u001b[39madjoint, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRayTransformAdjoint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m base_l \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorflow_layer\u001b[49m()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# User selected paramters\u001b[39;00m\n\u001b[1;32m      8\u001b[0m n_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'tensorflow_layer'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create tensorflow layer from odl operator\n",
    "odl_op_layer = odl.contrib.tensorflow.as_tensorflow_layer(operator,'RayTransform')\n",
    "odl_op_layer_adjoint = odl.contrib.tensorflow.as_tensorflow_layer(operator.adjoint, 'RayTransformAdjoint')\n",
    "\n",
    "base_l = tf.tensorflow_layer()\n",
    "\n",
    "# User selected paramters\n",
    "n_data = 20\n",
    "n_memory = 5\n",
    "n_iter = 10\n",
    "\n",
    "# Remove the placeholders and directly use variables for your data\n",
    "x_0 = tf.Variable(tf.zeros([n_data, size, size, 1]), dtype=tf.float32, name=\"x_0\")\n",
    "x_true = tf.Variable(tf.zeros([n_data, size, size, 1]), dtype=tf.float32, name=\"x_true\")\n",
    "y = tf.Variable(tf.zeros([n_data, operator.range.shape[0], operator.range.shape[1], 1]), dtype=tf.float32, name=\"y\")\n",
    "s = tf.Variable(tf.zeros([size, size, n_memory]), trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(validation=False):\n",
    "    \"\"\"Generate a set of random data.\"\"\"\n",
    "    n_iter = 1 if validation else n_data\n",
    "\n",
    "    x_arr = np.zeros((n_iter, space.shape[0], space.shape[1], 1), dtype=np.float32)\n",
    "    y_arr = np.zeros((n_iter, operator.range.shape[0], operator.range.shape[1], 1), dtype=np.float32)\n",
    "    x_true_arr = np.zeros((n_iter, space.shape[0], space.shape[1], 1), dtype=np.float32)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        if validation:\n",
    "            phantom = odl.phantom.shepp_logan(space, True)\n",
    "        else:\n",
    "            phantom = random_phantom(space)\n",
    "\n",
    "        data = operator(phantom)\n",
    "        noisy_data = data + odl.phantom.white_noise(operator.range) * np.mean(np.abs(data)) * 0.05\n",
    "        fbp = pseudoinverse(noisy_data)\n",
    "\n",
    "        x_arr[i, ..., 0] = fbp\n",
    "        x_true_arr[i, ..., 0] = phantom\n",
    "        y_arr[i, ..., 0] = noisy_data\n",
    "\n",
    "    return x_arr, y_arr, x_true_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_params:\n",
    "    \"\"\"\n",
    "    Initialization with Keras Initializers: \n",
    "    The tf.keras.initializers.GlorotUniform() is used to replace tf.contrib.layers.xavier_initializer_conv2d, \n",
    "    providing a more straightforward way to initialize weights.\n",
    "    \"\"\"\n",
    "    # Parameters if the network should be re-trained\n",
    "    w1 = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=[3, 3, n_memory + 2, 32]), name='w1')\n",
    "    b1 = tf.Variable(tf.constant(0.01, shape=[1, 1, 1, 32]), name='b1')\n",
    "\n",
    "    w2 = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=[3, 3, 32, 32]), name='w2')\n",
    "    b2 = tf.Variable(tf.constant(0.01, shape=[1, 1, 1, 32]), name='b2')\n",
    "\n",
    "    w3 = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=[3, 3, 32, n_memory + 1]), name='w3')\n",
    "    b3 = tf.Variable(tf.constant(0.00, shape=[1, 1, 1, n_memory + 1]), name='b3')\n",
    "else:\n",
    "    # If trained network is available, re-use as starting guess\n",
    "    ld = np.load(\"/Users/saityada/Desktop/coding/learned_gradient_tomography_addendum/code/partially_learned_gradient_descent_no_regularizer_parameters.npz\")\n",
    "\n",
    "    w1 = tf.Variable(tf.constant(ld['w1']), name='w1')\n",
    "    b1 = tf.Variable(tf.constant(ld['b1']), name='b1')\n",
    "\n",
    "    w2 = tf.Variable(tf.constant(ld['w2']), name='w2')\n",
    "    b2 = tf.Variable(tf.constant(ld['b2']), name='b2')\n",
    "\n",
    "    w3 = tf.Variable(tf.constant(ld['w3']), name='w3')\n",
    "    b3 = tf.Variable(tf.constant(ld['b3']), name='b3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"w1, {w1}, b1, {b1}\")\n",
    "print(f\"w2, {w2}, b2, {b2}\")\n",
    "print(f\"w3, {w3}, b3, {b3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def iterative_scheme(x_0, y, n_iter, s, w1, b1, w2, b2, w3, b3):\n",
    "    # Implementation of the iterative scheme\n",
    "    x_values = [x_0]\n",
    "    x = x_0\n",
    "    for i in range(n_iter):\n",
    "        with tf.name_scope(f'iterate_{i}'):\n",
    "            \n",
    "            # Compute the adjoint gradient\n",
    "            gradx = odl_op_layer_adjoint(odl_op_layer(x) - y)\n",
    "\n",
    "            # Concatenate and process updates\n",
    "            update = tf.concat([x, gradx, s], axis=3)\n",
    "\n",
    "            # Apply convolutions and activations\n",
    "            update = tf.nn.relu(conv2d(update, w1) + b1)\n",
    "            update = tf.nn.relu(conv2d(update, w2) + b2)\n",
    "            update = conv2d(update, w3) + b3\n",
    "\n",
    "            # Split the update into s and dx\n",
    "            s = tf.nn.relu(update[..., 1:])\n",
    "            dx = update[..., 0:1]\n",
    "\n",
    "            # Update x\n",
    "            x = x + dx\n",
    "            x_values.append(x)\n",
    "    return x_values, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validate_step(x_arr_validate, y_arr_validate, x_true_arr_validate):\n",
    "    # Get predictions using iterative_scheme\n",
    "    x_pred = iterative_scheme(x_0, y_arr_validate, n_iter, s, w1, b1, w2, b2, w3, b3)[-1]\n",
    "    # Compute loss\n",
    "    loss_result = tf.reduce_mean(tf.reduce_sum((x_pred - x_true_arr_validate) ** 2, axis=(1, 2)))\n",
    "    \n",
    "    return x_pred, loss_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the legacy optimizer\n",
    "from tensorflow.keras.optimizers.legacy import RMSprop\n",
    "\n",
    "# Learning rate\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step', dtype=tf.int64)\n",
    "starter_learning_rate = 1e-3\n",
    "\n",
    "# Create a learning rate schedule\n",
    "learning_rate_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    starter_learning_rate,\n",
    "    decay_steps=500,\n",
    "    decay_rate=1.0,\n",
    "    staircase=True,\n",
    "    name='learning_rate'\n",
    ")\n",
    "\n",
    "# Define the optimizer using Keras\n",
    "optimizer = RMSprop(learning_rate=learning_rate_schedule)\n",
    "\n",
    "# Solve with an ODL callback to see what happens in real time\n",
    "callback = odl.solvers.CallbackShow(clim=[0.1, 0.4])\n",
    "\n",
    "# Generate validation data\n",
    "x_arr_validate, y_arr_validate, x_true_arr_validate = generate_data(validation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_train:\n",
    "    # Train the network\n",
    "    n_train = 10\n",
    "    for i in range(n_train):\n",
    "        x_arr, y_arr, x_true_arr = generate_data()\n",
    "\n",
    "        # Use GradientTape to record operations\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_pred = iterative_scheme(x_0, y_arr, n_iter, s, w1, b1, w2, b2, w3, b3)[-1]  # Adjust as needed\n",
    "            loss_training = tf.reduce_mean(tf.reduce_sum((x_pred - x_true_arr) ** 2, axis=(1, 2))) # Compute the loss\n",
    "\n",
    "        gradients = tape.gradient(loss_training, [w1, w2, w3, b1, b2, b3])  # Compute gradients including any trainable variables\n",
    "        optimizer.apply_gradients(zip(gradients, [w1, w2, w3, b1, b2, b3])) # Update weights\n",
    "\n",
    "        # Validate on shepp-logan\n",
    "        x_values_result, loss_result = validate_step(x_arr_validate, y_arr_validate, x_true_arr_validate)  # Validate\n",
    "\n",
    "        print(\n",
    "            f'iter={i}, training loss={loss_training.numpy()} validation loss={loss_result.numpy()}'\n",
    "        )\n",
    "\n",
    "        callback((space ** (n_iter + 1)).element([xv.squeeze() for xv in x_values_result]))\n",
    "else:\n",
    "    # Validate on shepp-logan\n",
    "    x_values_result, loss_result = validate_step(x_arr_validate, y_arr_validate, x_true_arr_validate)  # Validate\n",
    "\n",
    "    print(f'validation loss={loss_result}')\n",
    "\n",
    "    callback((space ** (n_iter + 1)).element(\n",
    "        [xv.squeeze() for xv in x_values_result]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "illpropinverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
