{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jQ1tEQCxwRx"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:44.757710Z",
     "iopub.status.busy": "2024-08-16T06:32:44.757485Z",
     "iopub.status.idle": "2024-08-16T06:32:44.761332Z",
     "shell.execute_reply": "2024-08-16T06:32:44.760791Z"
    },
    "id": "V_sgB_5dx1f1"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF2x3qooyBTI"
   },
   "source": [
    "# Deep Convolutional Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TD5ZrvEMbhZ"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/dcgan\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/dcgan.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITZuApL56Mny"
   },
   "source": [
    "This tutorial demonstrates how to generate images of handwritten digits using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/pdf/1511.06434.pdf) (DCGAN). The code is written using the [Keras Sequential API](https://www.tensorflow.org/guide/keras) with a `tf.GradientTape` training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MbKJY38Puy9"
   },
   "source": [
    "## What are GANs?\n",
    "[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A *generator* (\"the artist\") learns to create images that look real, while a *discriminator* (\"the art critic\") learns to tell real images apart from fakes.\n",
    "\n",
    "![A diagram of a generator and discriminator](./images/gan1.png)\n",
    "\n",
    "During training, the *generator* progressively becomes better at creating images that look real, while the *discriminator* becomes better at telling them apart. The process reaches equilibrium when the *discriminator* can no longer distinguish real images from fakes.\n",
    "\n",
    "![A second diagram of a generator and discriminator](./images/gan2.png)\n",
    "\n",
    "This notebook demonstrates this process on the MNIST dataset. The following animation shows a series of images produced by the *generator* as it was trained for 50 epochs. The images begin as random noise, and increasingly resemble hand written digits over time.\n",
    "\n",
    "![sample output](https://tensorflow.org/images/gan/dcgan.gif)\n",
    "\n",
    "To learn more about GANs, see MIT's [Intro to Deep Learning](http://introtodeeplearning.com/) course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:44.764988Z",
     "iopub.status.busy": "2024-08-16T06:32:44.764768Z",
     "iopub.status.idle": "2024-08-16T06:32:47.117540Z",
     "shell.execute_reply": "2024-08-16T06:32:47.116811Z"
    },
    "id": "WZKbyU2-AiY-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import odl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:47.121498Z",
     "iopub.status.busy": "2024-08-16T06:32:47.121143Z",
     "iopub.status.idle": "2024-08-16T06:32:47.127684Z",
     "shell.execute_reply": "2024-08-16T06:32:47.127091Z"
    },
    "id": "wx-zNbLqB4K8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:47.130863Z",
     "iopub.status.busy": "2024-08-16T06:32:47.130467Z",
     "iopub.status.idle": "2024-08-16T06:32:52.456371Z",
     "shell.execute_reply": "2024-08-16T06:32:52.455227Z"
    },
    "id": "YzTlj4YdCip_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio\n",
      "  Using cached imageio-2.36.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy in /Users/saityada/miniconda3/lib/python3.12/site-packages (from imageio) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/saityada/miniconda3/lib/python3.12/site-packages (from imageio) (10.4.0)\n",
      "Using cached imageio-2.36.0-py3-none-any.whl (315 kB)\n",
      "Installing collected packages: imageio\n",
      "Successfully installed imageio-2.36.0\n",
      "Collecting git+https://github.com/tensorflow/docs\n",
      "  Cloning https://github.com/tensorflow/docs to /private/var/folders/c1/jljg7x_d7fb61svm70dxncg40000gn/T/pip-req-build-mwjx2zte\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs /private/var/folders/c1/jljg7x_d7fb61svm70dxncg40000gn/T/pip-req-build-mwjx2zte\n",
      "  Resolved https://github.com/tensorflow/docs to commit 15e96c3ee154d7c10ec9bb807ef44b4e6f08e65b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting astor (from tensorflow-docs==2024.10.14.18741)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py in /Users/saityada/miniconda3/lib/python3.12/site-packages (from tensorflow-docs==2024.10.14.18741) (2.1.0)\n",
      "Collecting jinja2 (from tensorflow-docs==2024.10.14.18741)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nbformat (from tensorflow-docs==2024.10.14.18741)\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: protobuf>=3.12 in /Users/saityada/miniconda3/lib/python3.12/site-packages (from tensorflow-docs==2024.10.14.18741) (5.28.3)\n",
      "Collecting pyyaml (from tensorflow-docs==2024.10.14.18741)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saityada/miniconda3/lib/python3.12/site-packages (from jinja2->tensorflow-docs==2024.10.14.18741) (3.0.2)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat->tensorflow-docs==2024.10.14.18741)\n",
      "  Using cached fastjsonschema-2.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting jsonschema>=2.6 (from nbformat->tensorflow-docs==2024.10.14.18741)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/saityada/miniconda3/lib/python3.12/site-packages (from nbformat->tensorflow-docs==2024.10.14.18741) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/saityada/miniconda3/lib/python3.12/site-packages (from nbformat->tensorflow-docs==2024.10.14.18741) (5.14.3)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.10.14.18741)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.10.14.18741)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.10.14.18741)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat->tensorflow-docs==2024.10.14.18741)\n",
      "  Downloading rpds_py-0.21.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/saityada/miniconda3/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow-docs==2024.10.14.18741) (3.10.0)\n",
      "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached fastjsonschema-2.20.0-py3-none-any.whl (23 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.21.0-cp312-cp312-macosx_11_0_arm64.whl (321 kB)\n",
      "Building wheels for collected packages: tensorflow-docs\n",
      "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorflow-docs: filename=tensorflow_docs-2024.10.14.18741-py3-none-any.whl size=182611 sha256=775d3cc384567205cd658e8ebbbfd42f77dcee29e6534df3c1620b2633c87db5\n",
      "  Stored in directory: /private/var/folders/c1/jljg7x_d7fb61svm70dxncg40000gn/T/pip-ephem-wheel-cache-m1mlnod3/wheels/3e/88/34/48d2789bc9d37b33ddce06bccc454fae0285e5396d0a5be9d9\n",
      "Successfully built tensorflow-docs\n",
      "Installing collected packages: fastjsonschema, rpds-py, pyyaml, jinja2, attrs, astor, referencing, jsonschema-specifications, jsonschema, nbformat, tensorflow-docs\n",
      "Successfully installed astor-0.8.1 attrs-24.2.0 fastjsonschema-2.20.0 jinja2-3.1.4 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 nbformat-5.10.4 pyyaml-6.0.2 referencing-0.35.1 rpds-py-0.21.0 tensorflow-docs-2024.10.14.18741\n"
     ]
    }
   ],
   "source": [
    "# To generate GIFs\n",
    "!pip install imageio\n",
    "!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:52.461040Z",
     "iopub.status.busy": "2024-08-16T06:32:52.460745Z",
     "iopub.status.idle": "2024-08-16T06:32:52.759966Z",
     "shell.execute_reply": "2024-08-16T06:32:52.759299Z"
    },
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - astra-toolbox\n",
      " - defaults\n",
      " - odlgroup\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/saityada/miniconda3\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    archspec-0.2.3             |     pyhd8ed1ab_0          48 KB  conda-forge\n",
      "    blas-2.125                 |         openblas          16 KB  conda-forge\n",
      "    blas-devel-3.9.0           |25_osxarm64_openblas          15 KB  conda-forge\n",
      "    boltons-24.0.0             |     pyhd8ed1ab_0         291 KB  conda-forge\n",
      "    brotli-1.1.0               |       hd74edd7_2          19 KB  conda-forge\n",
      "    brotli-bin-1.1.0           |       hd74edd7_2          16 KB  conda-forge\n",
      "    brotli-python-1.1.0        |  py312hde4cb15_2         331 KB  conda-forge\n",
      "    bzip2-1.0.8                |       h99b78c6_7         120 KB  conda-forge\n",
      "    c-ares-1.34.3              |       h5505292_0         175 KB  conda-forge\n",
      "    cffi-1.17.1                |  py312h0fad829_0         275 KB  conda-forge\n",
      "    charset-normalizer-3.4.0   |     pyhd8ed1ab_0          46 KB  conda-forge\n",
      "    colorama-0.4.6             |     pyhd8ed1ab_0          25 KB  conda-forge\n",
      "    conda-content-trust-0.2.0  |     pyhd8ed1ab_0          57 KB  conda-forge\n",
      "    conda-libmamba-solver-24.9.0|     pyhd8ed1ab_0          41 KB  conda-forge\n",
      "    conda-package-handling-2.4.0|     pyh7900ff3_0         252 KB  conda-forge\n",
      "    conda-package-streaming-0.11.0|     pyhd8ed1ab_0          20 KB  conda-forge\n",
      "    contourpy-1.3.1            |  py312hb23fbb9_0         240 KB  conda-forge\n",
      "    cryptography-43.0.3        |  py312h5fad481_0         1.3 MB  conda-forge\n",
      "    cycler-0.12.1              |     pyhd8ed1ab_0          13 KB  conda-forge\n",
      "    distro-1.9.0               |     pyhd8ed1ab_0          41 KB  conda-forge\n",
      "    expat-2.6.4                |       h286801f_0         122 KB  conda-forge\n",
      "    fmt-10.2.1                 |       h2ffa867_0         170 KB  conda-forge\n",
      "    fonttools-4.55.0           |  py312h998013c_0         2.6 MB  conda-forge\n",
      "    freetype-2.12.1            |       hadb7bae_2         582 KB  conda-forge\n",
      "    frozendict-2.4.6           |  py312h0bf5046_0          30 KB  conda-forge\n",
      "    future-1.0.0               |     pyhd8ed1ab_0         356 KB  conda-forge\n",
      "    h2-4.1.0                   |     pyhd8ed1ab_0          46 KB  conda-forge\n",
      "    hpack-4.0.0                |     pyh9f0ad1d_0          25 KB  conda-forge\n",
      "    hyperframe-6.0.1           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
      "    icu-73.2                   |       hc8870d7_0        11.4 MB  conda-forge\n",
      "    idna-3.10                  |     pyhd8ed1ab_0          49 KB  conda-forge\n",
      "    iniconfig-2.0.0            |     pyhd8ed1ab_0          11 KB  conda-forge\n",
      "    jpeg-9e                    |       h1a8c8d9_3         213 KB  conda-forge\n",
      "    jsonpatch-1.33             |     pyhd8ed1ab_0          17 KB  conda-forge\n",
      "    jsonpointer-3.0.0          |  py312h81bd7bf_1          17 KB  conda-forge\n",
      "    kiwisolver-1.4.7           |  py312h6142ec9_0          60 KB  conda-forge\n",
      "    krb5-1.21.3                |       h237132a_0         1.1 MB  conda-forge\n",
      "    lcms2-2.15                 |       h481adae_0         201 KB  conda-forge\n",
      "    lerc-4.0.0                 |       h9a09cb3_0         211 KB  conda-forge\n",
      "    libarchive-3.7.4           |       h83d404f_0         758 KB  conda-forge\n",
      "    libblas-3.9.0              |25_osxarm64_openblas          16 KB  conda-forge\n",
      "    libbrotlicommon-1.1.0      |       hd74edd7_2          67 KB  conda-forge\n",
      "    libbrotlidec-1.1.0         |       hd74edd7_2          28 KB  conda-forge\n",
      "    libbrotlienc-1.1.0         |       hd74edd7_2         273 KB  conda-forge\n",
      "    libcblas-3.9.0             |25_osxarm64_openblas          15 KB  conda-forge\n",
      "    libcurl-8.8.0              |       h7b6f9a7_1         361 KB  conda-forge\n",
      "    libdeflate-1.17            |       h1a8c8d9_0          47 KB  conda-forge\n",
      "    libev-4.33                 |       h93a5062_2         105 KB  conda-forge\n",
      "    libexpat-2.6.4             |       h286801f_0          63 KB  conda-forge\n",
      "    libgfortran-5.0.0          |13_2_0_hd922786_3         108 KB  conda-forge\n",
      "    libgfortran5-13.2.0        |       hf226fd6_3         974 KB  conda-forge\n",
      "    libiconv-1.17              |       h0d3ecfb_2         661 KB  conda-forge\n",
      "    liblapack-3.9.0            |25_osxarm64_openblas          15 KB  conda-forge\n",
      "    liblapacke-3.9.0           |25_osxarm64_openblas          15 KB  conda-forge\n",
      "    libmamba-1.5.8             |       h90c426b_0         1.1 MB  conda-forge\n",
      "    libmambapy-1.5.8           |  py312h344e357_0         249 KB  conda-forge\n",
      "    libnghttp2-1.58.0          |       ha4dd798_1         552 KB  conda-forge\n",
      "    libopenblas-0.3.28         |openmp_hf332438_1         4.0 MB  conda-forge\n",
      "    libpng-1.6.43              |       h091b4b1_0         258 KB  conda-forge\n",
      "    libsodium-1.0.20           |       h99b78c6_0         161 KB  conda-forge\n",
      "    libsolv-0.7.29             |       h1efcc80_0         380 KB  conda-forge\n",
      "    libssh2-1.11.0             |       h7a5bd25_0         250 KB  conda-forge\n",
      "    libtiff-4.5.0              |       h5dffbdd_2         339 KB  conda-forge\n",
      "    libwebp-base-1.4.0         |       h93a5062_0         281 KB  conda-forge\n",
      "    libxml2-2.12.7             |       ha661575_1         575 KB  conda-forge\n",
      "    llvm-openmp-19.1.3         |       hb52a8e5_0         274 KB  conda-forge\n",
      "    lz4-c-1.9.4                |       hb7217d7_0         138 KB  conda-forge\n",
      "    lzo-2.10                   |    h93a5062_1001         128 KB  conda-forge\n",
      "    matplotlib-3.9.2           |  py312h1f38498_2          17 KB  conda-forge\n",
      "    matplotlib-base-3.9.2      |  py312h9bd0bc6_2         7.4 MB  conda-forge\n",
      "    menuinst-2.2.0             |  py312h81bd7bf_0         163 KB  conda-forge\n",
      "    munkres-1.1.4              |     pyh9f0ad1d_0          12 KB  conda-forge\n",
      "    ncurses-6.5                |       h7bae524_1         784 KB  conda-forge\n",
      "    odl-0.8.0                  |     pyhd8ed1ab_0         499 KB  conda-forge\n",
      "    openblas-0.3.28            |openmp_hea878ba_1         2.9 MB  conda-forge\n",
      "    packaging-24.2             |     pyhff2d567_1          59 KB  conda-forge\n",
      "    pcre2-10.43                |       h26f9a81_0         601 KB  conda-forge\n",
      "    pip-24.3.1                 |     pyh8b19718_0         1.2 MB  conda-forge\n",
      "    pluggy-1.5.0               |     pyhd8ed1ab_0          23 KB  conda-forge\n",
      "    pybind11-abi-4             |       hd8ed1ab_3          10 KB  conda-forge\n",
      "    pycosat-0.6.6              |  py312h02f2b3b_0          84 KB  conda-forge\n",
      "    pycparser-2.22             |     pyhd8ed1ab_0         103 KB  conda-forge\n",
      "    pyparsing-3.2.0            |     pyhd8ed1ab_1          90 KB  conda-forge\n",
      "    pysocks-1.7.1              |     pyha2e5f31_6          19 KB  conda-forge\n",
      "    pytest-8.3.3               |     pyhd8ed1ab_0         252 KB  conda-forge\n",
      "    pyzmq-26.2.0               |  py312hf8a1cbd_3         353 KB  conda-forge\n",
      "    qhull-2020.2               |       h420ef59_5         504 KB  conda-forge\n",
      "    readline-8.2               |       h92ec313_1         244 KB  conda-forge\n",
      "    reproc-14.2.4.post0        |       h93a5062_1          31 KB  conda-forge\n",
      "    reproc-cpp-14.2.4.post0    |       h965bd2d_1          24 KB  conda-forge\n",
      "    requests-2.32.3            |     pyhd8ed1ab_0          57 KB  conda-forge\n",
      "    ruamel.yaml-0.18.6         |  py312h0bf5046_1         262 KB  conda-forge\n",
      "    ruamel.yaml.clib-0.2.8     |  py312h0bf5046_1         114 KB  conda-forge\n",
      "    scipy-1.14.1               |  py312h20deb59_1        14.4 MB  conda-forge\n",
      "    setuptools-75.5.0          |     pyhff2d567_0         754 KB  conda-forge\n",
      "    six-1.16.0                 |     pyh6c4a22f_0          14 KB  conda-forge\n",
      "    sqlite-3.46.0              |       h5838104_0         803 KB  conda-forge\n",
      "    tomli-2.1.0                |     pyhff2d567_0          18 KB  conda-forge\n",
      "    tornado-6.4.1              |  py312h024a12e_1         822 KB  conda-forge\n",
      "    tqdm-4.67.0                |     pyhd8ed1ab_0          87 KB  conda-forge\n",
      "    truststore-0.10.0          |     pyhd8ed1ab_0          21 KB  conda-forge\n",
      "    tzdata-2024b               |       hc8b5060_0         119 KB  conda-forge\n",
      "    unicodedata2-15.1.0        |  py312h0bf5046_1         364 KB  conda-forge\n",
      "    urllib3-2.2.3              |     pyhd8ed1ab_0          96 KB  conda-forge\n",
      "    wheel-0.45.0               |     pyhd8ed1ab_0          61 KB  conda-forge\n",
      "    yaml-cpp-0.8.0             |       h13dd4ca_0         127 KB  conda-forge\n",
      "    zeromq-4.3.5               |       hc1bb282_7         275 KB  conda-forge\n",
      "    zstandard-0.23.0           |  py312h15fbf35_1         323 KB  conda-forge\n",
      "    zstd-1.5.6                 |       hb46c0d2_0         396 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        66.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas-devel         conda-forge/osx-arm64::blas-devel-3.9.0-25_osxarm64_openblas \n",
      "  colorama           conda-forge/noarch::colorama-0.4.6-pyhd8ed1ab_0 \n",
      "  h2                 conda-forge/noarch::h2-4.1.0-pyhd8ed1ab_0 \n",
      "  hpack              conda-forge/noarch::hpack-4.0.0-pyh9f0ad1d_0 \n",
      "  hyperframe         conda-forge/noarch::hyperframe-6.0.1-pyhd8ed1ab_0 \n",
      "  libblas            conda-forge/osx-arm64::libblas-3.9.0-25_osxarm64_openblas \n",
      "  libcblas           conda-forge/osx-arm64::libcblas-3.9.0-25_osxarm64_openblas \n",
      "  liblapack          conda-forge/osx-arm64::liblapack-3.9.0-25_osxarm64_openblas \n",
      "  liblapacke         conda-forge/osx-arm64::liblapacke-3.9.0-25_osxarm64_openblas \n",
      "  lzo                conda-forge/osx-arm64::lzo-2.10-h93a5062_1001 \n",
      "  munkres            conda-forge/noarch::munkres-1.1.4-pyh9f0ad1d_0 \n",
      "  openblas           conda-forge/osx-arm64::openblas-0.3.28-openmp_hea878ba_1 \n",
      "  qhull              conda-forge/osx-arm64::qhull-2020.2-h420ef59_5 \n",
      "  ruamel.yaml.clib   conda-forge/osx-arm64::ruamel.yaml.clib-0.2.8-py312h0bf5046_1 \n",
      "  tomli              conda-forge/noarch::tomli-2.1.0-pyhff2d567_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  blas                         pkgs/main::blas-1.0-openblas --> conda-forge::blas-2.125-openblas \n",
      "  boltons            pkgs/main/osx-arm64::boltons-23.0.0-p~ --> conda-forge/noarch::boltons-24.0.0-pyhd8ed1ab_0 \n",
      "  brotli                 pkgs/main::brotli-1.0.9-h80987f9_8 --> conda-forge::brotli-1.1.0-hd74edd7_2 \n",
      "  brotli-bin         pkgs/main::brotli-bin-1.0.9-h80987f9_8 --> conda-forge::brotli-bin-1.1.0-hd74edd7_2 \n",
      "  brotli-python      pkgs/main::brotli-python-1.0.9-py312h~ --> conda-forge::brotli-python-1.1.0-py312hde4cb15_2 \n",
      "  bzip2                   pkgs/main::bzip2-1.0.8-h80987f9_6 --> conda-forge::bzip2-1.0.8-h99b78c6_7 \n",
      "  c-ares                pkgs/main::c-ares-1.19.1-h80987f9_0 --> conda-forge::c-ares-1.34.3-h5505292_0 \n",
      "  cffi               pkgs/main::cffi-1.16.0-py312h80987f9_1 --> conda-forge::cffi-1.17.1-py312h0fad829_0 \n",
      "  charset-normalizer pkgs/main::charset-normalizer-3.3.2-p~ --> conda-forge::charset-normalizer-3.4.0-pyhd8ed1ab_0 \n",
      "  conda-libmamba-so~ pkgs/main::conda-libmamba-solver-24.7~ --> conda-forge::conda-libmamba-solver-24.9.0-pyhd8ed1ab_0 \n",
      "  conda-package-han~ pkgs/main/osx-arm64::conda-package-ha~ --> conda-forge/noarch::conda-package-handling-2.4.0-pyh7900ff3_0 \n",
      "  conda-package-str~ pkgs/main/osx-arm64::conda-package-st~ --> conda-forge/noarch::conda-package-streaming-0.11.0-pyhd8ed1ab_0 \n",
      "  contourpy          pkgs/main::contourpy-1.2.0-py312h48ca~ --> conda-forge::contourpy-1.3.1-py312hb23fbb9_0 \n",
      "  cryptography       pkgs/main::cryptography-42.0.5-py312h~ --> conda-forge::cryptography-43.0.3-py312h5fad481_0 \n",
      "  cycler              pkgs/main::cycler-0.11.0-pyhd3eb1b0_0 --> conda-forge::cycler-0.12.1-pyhd8ed1ab_0 \n",
      "  expat                   pkgs/main::expat-2.6.2-h313beb8_0 --> conda-forge::expat-2.6.4-h286801f_0 \n",
      "  fmt                       pkgs/main::fmt-9.1.0-h48ca7d4_1 --> conda-forge::fmt-10.2.1-h2ffa867_0 \n",
      "  fonttools          pkgs/main::fonttools-4.51.0-py312h809~ --> conda-forge::fonttools-4.55.0-py312h998013c_0 \n",
      "  freetype            pkgs/main::freetype-2.12.1-h1192e45_0 --> conda-forge::freetype-2.12.1-hadb7bae_2 \n",
      "  frozendict         pkgs/main::frozendict-2.4.2-py312hca0~ --> conda-forge::frozendict-2.4.6-py312h0bf5046_0 \n",
      "  future             pkgs/main/osx-arm64::future-0.18.3-py~ --> conda-forge/noarch::future-1.0.0-pyhd8ed1ab_0 \n",
      "  icu                        pkgs/main::icu-73.1-h313beb8_0 --> conda-forge::icu-73.2-hc8870d7_0 \n",
      "  idna               pkgs/main/osx-arm64::idna-3.7-py312hc~ --> conda-forge/noarch::idna-3.10-pyhd8ed1ab_0 \n",
      "  iniconfig          pkgs/main::iniconfig-1.1.1-pyhd3eb1b0~ --> conda-forge::iniconfig-2.0.0-pyhd8ed1ab_0 \n",
      "  jsonpointer        pkgs/main/noarch::jsonpointer-2.1-pyh~ --> conda-forge/osx-arm64::jsonpointer-3.0.0-py312h81bd7bf_1 \n",
      "  kiwisolver         pkgs/main::kiwisolver-1.4.4-py312h313~ --> conda-forge::kiwisolver-1.4.7-py312h6142ec9_0 \n",
      "  krb5                    pkgs/main::krb5-1.20.1-hf3e1bf2_1 --> conda-forge::krb5-1.21.3-h237132a_0 \n",
      "  lcms2                    pkgs/main::lcms2-2.12-hba8e193_0 --> conda-forge::lcms2-2.15-h481adae_0 \n",
      "  lerc                       pkgs/main::lerc-3.0-hc377ac9_0 --> conda-forge::lerc-4.0.0-h9a09cb3_0 \n",
      "  libarchive         pkgs/main::libarchive-3.6.2-h62fee54_3 --> conda-forge::libarchive-3.7.4-h83d404f_0 \n",
      "  libbrotlicommon    pkgs/main::libbrotlicommon-1.0.9-h809~ --> conda-forge::libbrotlicommon-1.1.0-hd74edd7_2 \n",
      "  libbrotlidec       pkgs/main::libbrotlidec-1.0.9-h80987f~ --> conda-forge::libbrotlidec-1.1.0-hd74edd7_2 \n",
      "  libbrotlienc       pkgs/main::libbrotlienc-1.0.9-h80987f~ --> conda-forge::libbrotlienc-1.1.0-hd74edd7_2 \n",
      "  libcurl               pkgs/main::libcurl-8.7.1-h3e2b118_0 --> conda-forge::libcurl-8.8.0-h7b6f9a7_1 \n",
      "  libev                    pkgs/main::libev-4.33-h1a28f6b_1 --> conda-forge::libev-4.33-h93a5062_2 \n",
      "  libexpat                                 2.6.2-hebf3989_0 --> 2.6.4-h286801f_0 \n",
      "  libgfortran5       pkgs/main::libgfortran5-11.3.0-h00934~ --> conda-forge::libgfortran5-13.2.0-hf226fd6_3 \n",
      "  libiconv              pkgs/main::libiconv-1.16-h80987f9_3 --> conda-forge::libiconv-1.17-h0d3ecfb_2 \n",
      "  libnghttp2         pkgs/main::libnghttp2-1.57.0-h62f6fdd~ --> conda-forge::libnghttp2-1.58.0-ha4dd798_1 \n",
      "  libopenblas        pkgs/main::libopenblas-0.3.21-h269037~ --> conda-forge::libopenblas-0.3.28-openmp_hf332438_1 \n",
      "  libpng                pkgs/main::libpng-1.6.39-h80987f9_0 --> conda-forge::libpng-1.6.43-h091b4b1_0 \n",
      "  libsodium                               1.0.18-h27ca646_1 --> 1.0.20-h99b78c6_0 \n",
      "  libsolv              pkgs/main::libsolv-0.7.24-h514c7bf_1 --> conda-forge::libsolv-0.7.29-h1efcc80_0 \n",
      "  libwebp-base       pkgs/main::libwebp-base-1.3.2-h80987f~ --> conda-forge::libwebp-base-1.4.0-h93a5062_0 \n",
      "  libxml2              pkgs/main::libxml2-2.10.4-h0b34f26_2 --> conda-forge::libxml2-2.12.7-ha661575_1 \n",
      "  llvm-openmp        pkgs/main::llvm-openmp-14.0.6-hc6e570~ --> conda-forge::llvm-openmp-19.1.3-hb52a8e5_0 \n",
      "  matplotlib         pkgs/main::matplotlib-3.9.2-py312hca0~ --> conda-forge::matplotlib-3.9.2-py312h1f38498_2 \n",
      "  matplotlib-base    pkgs/main::matplotlib-base-3.9.2-py31~ --> conda-forge::matplotlib-base-3.9.2-py312h9bd0bc6_2 \n",
      "  menuinst           pkgs/main::menuinst-2.1.2-py312hca03d~ --> conda-forge::menuinst-2.2.0-py312h81bd7bf_0 \n",
      "  ncurses                 pkgs/main::ncurses-6.4-h313beb8_0 --> conda-forge::ncurses-6.5-h7bae524_1 \n",
      "  odl                              odlgroup::odl-0.5.1-py_5 --> conda-forge::odl-0.8.0-pyhd8ed1ab_0 \n",
      "  packaging          pkgs/main/osx-arm64::packaging-24.1-p~ --> conda-forge/noarch::packaging-24.2-pyhff2d567_1 \n",
      "  pcre2                   pkgs/main::pcre2-10.42-hb066dcc_1 --> conda-forge::pcre2-10.43-h26f9a81_0 \n",
      "  pip                pkgs/main/osx-arm64::pip-24.2-py312hc~ --> conda-forge/noarch::pip-24.3.1-pyh8b19718_0 \n",
      "  platformdirs       pkgs/main/osx-arm64::platformdirs-3.1~ --> conda-forge/noarch::platformdirs-4.3.6-pyhd8ed1ab_0 \n",
      "  pluggy             pkgs/main/osx-arm64::pluggy-1.0.0-py3~ --> conda-forge/noarch::pluggy-1.5.0-pyhd8ed1ab_0 \n",
      "  pycparser          pkgs/main::pycparser-2.21-pyhd3eb1b0_0 --> conda-forge::pycparser-2.22-pyhd8ed1ab_0 \n",
      "  pyparsing          pkgs/main/osx-arm64::pyparsing-3.1.2-~ --> conda-forge/noarch::pyparsing-3.2.0-pyhd8ed1ab_1 \n",
      "  pysocks            pkgs/main/osx-arm64::pysocks-1.7.1-py~ --> conda-forge/noarch::pysocks-1.7.1-pyha2e5f31_6 \n",
      "  pytest             pkgs/main/osx-arm64::pytest-7.4.4-py3~ --> conda-forge/noarch::pytest-8.3.3-pyhd8ed1ab_0 \n",
      "  pyzmq                              26.2.0-py312hc6335d2_1 --> 26.2.0-py312hf8a1cbd_3 \n",
      "  readline               pkgs/main::readline-8.2-h1a28f6b_0 --> conda-forge::readline-8.2-h92ec313_1 \n",
      "  reproc                pkgs/main::reproc-14.2.4-h313beb8_2 --> conda-forge::reproc-14.2.4.post0-h93a5062_1 \n",
      "  reproc-cpp         pkgs/main::reproc-cpp-14.2.4-h313beb8~ --> conda-forge::reproc-cpp-14.2.4.post0-h965bd2d_1 \n",
      "  ruamel.yaml        pkgs/main::ruamel.yaml-0.17.21-py312h~ --> conda-forge::ruamel.yaml-0.18.6-py312h0bf5046_1 \n",
      "  scipy              pkgs/main::scipy-1.13.1-py312ha409365~ --> conda-forge::scipy-1.14.1-py312h20deb59_1 \n",
      "  setuptools         pkgs/main/osx-arm64::setuptools-72.1.~ --> conda-forge/noarch::setuptools-75.5.0-pyhff2d567_0 \n",
      "  sqlite                pkgs/main::sqlite-3.45.3-h80987f9_0 --> conda-forge::sqlite-3.46.0-h5838104_0 \n",
      "  tornado            pkgs/main::tornado-6.4.1-py312h80987f~ --> conda-forge::tornado-6.4.1-py312h024a12e_1 \n",
      "  tqdm               pkgs/main/osx-arm64::tqdm-4.66.4-py31~ --> conda-forge/noarch::tqdm-4.67.0-pyhd8ed1ab_0 \n",
      "  truststore         pkgs/main/osx-arm64::truststore-0.8.0~ --> conda-forge/noarch::truststore-0.10.0-pyhd8ed1ab_0 \n",
      "  tzdata                 pkgs/main::tzdata-2024a-h04d1e81_0 --> conda-forge::tzdata-2024b-hc8b5060_0 \n",
      "  unicodedata2       pkgs/main::unicodedata2-15.1.0-py312h~ --> conda-forge::unicodedata2-15.1.0-py312h0bf5046_1 \n",
      "  urllib3            pkgs/main/osx-arm64::urllib3-2.2.2-py~ --> conda-forge/noarch::urllib3-2.2.3-pyhd8ed1ab_0 \n",
      "  wheel              pkgs/main/osx-arm64::wheel-0.43.0-py3~ --> conda-forge/noarch::wheel-0.45.0-pyhd8ed1ab_0 \n",
      "  zeromq                                   4.3.5-hebf3989_1 --> 4.3.5-hc1bb282_7 \n",
      "  zstandard          pkgs/main::zstandard-0.22.0-py312h1a4~ --> conda-forge::zstandard-0.23.0-py312h15fbf35_1 \n",
      "  zstd                     pkgs/main::zstd-1.5.5-hd90d995_2 --> conda-forge::zstd-1.5.6-hb46c0d2_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  archspec           pkgs/main::archspec-0.2.3-pyhd3eb1b0_0 --> conda-forge::archspec-0.2.3-pyhd8ed1ab_0 \n",
      "  conda-content-tru~ pkgs/main/osx-arm64::conda-content-tr~ --> conda-forge/noarch::conda-content-trust-0.2.0-pyhd8ed1ab_0 \n",
      "  distro             pkgs/main/osx-arm64::distro-1.9.0-py3~ --> conda-forge/noarch::distro-1.9.0-pyhd8ed1ab_0 \n",
      "  jpeg                        pkgs/main::jpeg-9e-h80987f9_3 --> conda-forge::jpeg-9e-h1a8c8d9_3 \n",
      "  jsonpatch          pkgs/main/osx-arm64::jsonpatch-1.33-p~ --> conda-forge/noarch::jsonpatch-1.33-pyhd8ed1ab_0 \n",
      "  libdeflate          pkgs/main::libdeflate-1.17-h80987f9_1 --> conda-forge::libdeflate-1.17-h1a8c8d9_0 \n",
      "  libgfortran        pkgs/main::libgfortran-5.0.0-11_3_0_h~ --> conda-forge::libgfortran-5.0.0-13_2_0_hd922786_3 \n",
      "  libmamba             pkgs/main::libmamba-1.5.8-haeffa04_2 --> conda-forge::libmamba-1.5.8-h90c426b_0 \n",
      "  libmambapy         pkgs/main::libmambapy-1.5.8-py312h1c5~ --> conda-forge::libmambapy-1.5.8-py312h344e357_0 \n",
      "  libssh2              pkgs/main::libssh2-1.11.0-h3e2b118_0 --> conda-forge::libssh2-1.11.0-h7a5bd25_0 \n",
      "  libtiff               pkgs/main::libtiff-4.5.1-h313beb8_0 --> conda-forge::libtiff-4.5.0-h5dffbdd_2 \n",
      "  lz4-c                   pkgs/main::lz4-c-1.9.4-h313beb8_1 --> conda-forge::lz4-c-1.9.4-hb7217d7_0 \n",
      "  pybind11-abi         pkgs/main::pybind11-abi-5-hd3eb1b0_0 --> conda-forge::pybind11-abi-4-hd8ed1ab_3 \n",
      "  pycosat            pkgs/main::pycosat-0.6.6-py312h80987f~ --> conda-forge::pycosat-0.6.6-py312h02f2b3b_0 \n",
      "  requests           pkgs/main/osx-arm64::requests-2.32.3-~ --> conda-forge/noarch::requests-2.32.3-pyhd8ed1ab_0 \n",
      "  six                    pkgs/main::six-1.16.0-pyhd3eb1b0_1 --> conda-forge::six-1.16.0-pyh6c4a22f_0 \n",
      "  yaml-cpp             pkgs/main::yaml-cpp-0.8.0-h313beb8_1 --> conda-forge::yaml-cpp-0.8.0-h13dd4ca_0 \n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda update --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "### Load and prepare the dataset\n",
    "\n",
    "You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:52.763882Z",
     "iopub.status.busy": "2024-08-16T06:32:52.763618Z",
     "iopub.status.idle": "2024-08-16T06:32:53.032357Z",
     "shell.execute_reply": "2024-08-16T06:32:53.031648Z"
    },
    "id": "a4fYMGxGhrna"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/urllib/request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1344\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1377\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1326\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1326\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1085\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1085\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1088\u001b[0m \n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1029\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1472\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1470\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m-> 1472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/utils/file_utils.py:309\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/urllib/request.py:240\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    238\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    241\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/urllib/request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/urllib/request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    533\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    491\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/urllib/request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/urllib/request.py:1347\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1348\u001b[0m r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (train_images, train_labels), (_, _) \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmnist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/datasets/mnist.py:60\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the MNIST dataset.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mThis is a dataset of 60,000 28x28 grayscale images of the 10 digits,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    https://creativecommons.org/licenses/by-sa/3.0/)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m origin_folder \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/tensorflow/tf-keras-datasets/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnist.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     68\u001b[0m     x_train, y_train \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_train\u001b[39m\u001b[38;5;124m\"\u001b[39m], f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/utils/file_utils.py:313\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmsg))\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 313\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg\u001b[38;5;241m.\u001b[39mformat(origin, e\u001b[38;5;241m.\u001b[39merrno, e\u001b[38;5;241m.\u001b[39mreason))\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(download_target):\n",
      "\u001b[0;31mException\u001b[0m: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:53.036481Z",
     "iopub.status.busy": "2024-08-16T06:32:53.036232Z",
     "iopub.status.idle": "2024-08-16T06:32:53.184845Z",
     "shell.execute_reply": "2024-08-16T06:32:53.183995Z"
    },
    "id": "NFC2ghIdiZYE"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:53.188894Z",
     "iopub.status.busy": "2024-08-16T06:32:53.188627Z",
     "iopub.status.idle": "2024-08-16T06:32:53.191907Z",
     "shell.execute_reply": "2024-08-16T06:32:53.191314Z"
    },
    "id": "S4PIDhoDLbsZ"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:53.194978Z",
     "iopub.status.busy": "2024-08-16T06:32:53.194725Z",
     "iopub.status.idle": "2024-08-16T06:32:55.906143Z",
     "shell.execute_reply": "2024-08-16T06:32:55.905444Z"
    },
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Create the models\n",
    "\n",
    "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tEyxE-GMC48"
   },
   "source": [
    "### The Generator\n",
    "\n",
    "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:55.909709Z",
     "iopub.status.busy": "2024-08-16T06:32:55.909461Z",
     "iopub.status.idle": "2024-08-16T06:32:55.916374Z",
     "shell.execute_reply": "2024-08-16T06:32:55.915794Z"
    },
    "id": "6bpTcDqoLWjY"
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWgG09LCSJl"
   },
   "source": [
    "Use the (as yet untrained) generator to create an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:55.919460Z",
     "iopub.status.busy": "2024-08-16T06:32:55.919235Z",
     "iopub.status.idle": "2024-08-16T06:32:57.342189Z",
     "shell.execute_reply": "2024-08-16T06:32:57.341343Z"
    },
    "id": "gl7jcC7TdPTG"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "### The Discriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.345774Z",
     "iopub.status.busy": "2024-08-16T06:32:57.345495Z",
     "iopub.status.idle": "2024-08-16T06:32:57.351061Z",
     "shell.execute_reply": "2024-08-16T06:32:57.350236Z"
    },
    "id": "dw2tPLmk2pEP"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhPneagzCaQv"
   },
   "source": [
    "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.354335Z",
     "iopub.status.busy": "2024-08-16T06:32:57.354031Z",
     "iopub.status.idle": "2024-08-16T06:32:57.500096Z",
     "shell.execute_reply": "2024-08-16T06:32:57.499239Z"
    },
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss and optimizers\n",
    "\n",
    "Define loss functions and optimizers for both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.503576Z",
     "iopub.status.busy": "2024-08-16T06:32:57.503309Z",
     "iopub.status.idle": "2024-08-16T06:32:57.507294Z",
     "shell.execute_reply": "2024-08-16T06:32:57.506464Z"
    },
    "id": "psQfmXxYKU3X"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKY_iPSPNWoj"
   },
   "source": [
    "### Discriminator loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.510537Z",
     "iopub.status.busy": "2024-08-16T06:32:57.510289Z",
     "iopub.status.idle": "2024-08-16T06:32:57.514512Z",
     "shell.execute_reply": "2024-08-16T06:32:57.513515Z"
    },
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd-3GCUEiKtv"
   },
   "source": [
    "### Generator loss\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminators decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.518013Z",
     "iopub.status.busy": "2024-08-16T06:32:57.517769Z",
     "iopub.status.idle": "2024-08-16T06:32:57.521551Z",
     "shell.execute_reply": "2024-08-16T06:32:57.520716Z"
    },
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgIc7i0th_Iu"
   },
   "source": [
    "The discriminator and the generator optimizers are different since you will train two networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.524992Z",
     "iopub.status.busy": "2024-08-16T06:32:57.524720Z",
     "iopub.status.idle": "2024-08-16T06:32:57.533903Z",
     "shell.execute_reply": "2024-08-16T06:32:57.533033Z"
    },
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "### Save checkpoints\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.537163Z",
     "iopub.status.busy": "2024-08-16T06:32:57.536913Z",
     "iopub.status.idle": "2024-08-16T06:32:57.541545Z",
     "shell.execute_reply": "2024-08-16T06:32:57.540721Z"
    },
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Define the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.544891Z",
     "iopub.status.busy": "2024-08-16T06:32:57.544612Z",
     "iopub.status.idle": "2024-08-16T06:32:57.549427Z",
     "shell.execute_reply": "2024-08-16T06:32:57.548601Z"
    },
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jylSonrqSWfi"
   },
   "source": [
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.552856Z",
     "iopub.status.busy": "2024-08-16T06:32:57.552612Z",
     "iopub.status.idle": "2024-08-16T06:32:57.559063Z",
     "shell.execute_reply": "2024-08-16T06:32:57.558200Z"
    },
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.562101Z",
     "iopub.status.busy": "2024-08-16T06:32:57.561856Z",
     "iopub.status.idle": "2024-08-16T06:32:57.567177Z",
     "shell.execute_reply": "2024-08-16T06:32:57.566349Z"
    },
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFF7Hk3XdeW"
   },
   "source": [
    "**Generate and save images**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.570588Z",
     "iopub.status.busy": "2024-08-16T06:32:57.570323Z",
     "iopub.status.idle": "2024-08-16T06:32:57.575468Z",
     "shell.execute_reply": "2024-08-16T06:32:57.574502Z"
    },
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZrd4CdjR-Fp"
   },
   "source": [
    "## Train the model\n",
    "Call the `train()` method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:32:57.578834Z",
     "iopub.status.busy": "2024-08-16T06:32:57.578578Z",
     "iopub.status.idle": "2024-08-16T06:44:20.513797Z",
     "shell.execute_reply": "2024-08-16T06:44:20.513090Z"
    },
    "id": "Ly3UN0SLLY2l"
   },
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfM4YcPVPkNO"
   },
   "source": [
    "Restore the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:44:20.517311Z",
     "iopub.status.busy": "2024-08-16T06:44:20.517053Z",
     "iopub.status.idle": "2024-08-16T06:44:20.590831Z",
     "shell.execute_reply": "2024-08-16T06:44:20.590175Z"
    },
    "id": "XhXsd0srPo8c"
   },
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "## Create a GIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:44:20.594108Z",
     "iopub.status.busy": "2024-08-16T06:44:20.593854Z",
     "iopub.status.idle": "2024-08-16T06:44:20.597342Z",
     "shell.execute_reply": "2024-08-16T06:44:20.596744Z"
    },
    "id": "WfO5wCdclHGL"
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:44:20.600499Z",
     "iopub.status.busy": "2024-08-16T06:44:20.599954Z",
     "iopub.status.idle": "2024-08-16T06:44:20.615535Z",
     "shell.execute_reply": "2024-08-16T06:44:20.614982Z"
    },
    "id": "5x3q9_Oe5q0A"
   },
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NywiH3nL8guF"
   },
   "source": [
    "Use `imageio` to create an animated gif using the images saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:44:20.618960Z",
     "iopub.status.busy": "2024-08-16T06:44:20.618364Z",
     "iopub.status.idle": "2024-08-16T06:44:21.202896Z",
     "shell.execute_reply": "2024-08-16T06:44:21.202120Z"
    },
    "id": "IGKQgENQ8lEI"
   },
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T06:44:21.207070Z",
     "iopub.status.busy": "2024-08-16T06:44:21.206432Z",
     "iopub.status.idle": "2024-08-16T06:44:21.226514Z",
     "shell.execute_reply": "2024-08-16T06:44:21.225919Z"
    },
    "id": "ZBwyU6t2Wf3g"
   },
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6qC-SbjK0yW"
   },
   "source": [
    "## Next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjjkT9KAK6H7"
   },
   "source": [
    "This tutorial has shown the complete code necessary to write and train a GAN. As a next step, you might like to experiment with a different dataset, for example the Large-scale Celeb Faces Attributes (CelebA) dataset [available on Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset). To learn more about GANs see the [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
